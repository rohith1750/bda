client :
  
create input file: sudo gedit input.txt

paste input: vi input.txt

hdfs dfs -mkdir /rohith
hdfs dfs -put input.txt /rohith/



master:

create pig file
sudo gedit sample.pig
enter pass:hdoop

in pig script 
data = LOAD '/rohith/input.txt' USING PigStorage(',') AS (id:int, name:chararray, date:chararray, number:int);

data = LOAD 'input_file' AS (userid: chararray, timestamp: long, action: chararray);

-- Group by userid and action
grouped = GROUP data BY userid, action;

-- Count the number of actions for each user-action combination
counted = FOREACH grouped GENERATE userid, action, COUNT(data) AS action_count;

-- Filter users with any action count greater than or equal to 3
filtered = FILTER counted BY action_count >= 3;

-- Project the user ID
result = FOREACH filtered GENERATE userid;

-- Store the results
STORE result INTO '/rohith/out';











run it using :pig -x mapreduce sample.pig


pig -x local sample.pig



